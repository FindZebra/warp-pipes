---
title: Interoperability
author: Benjamin Starostka Jakobsen
status: draft
---

# Overview of different types of pipes
**Processing pipes:**
Identity, Lambda, GetKey, FilterKeys, DropKeys, AddPrefix, ReplaceInKeys, RenameKeys, Apply, ApplyToAll, CopyBatch, Partial

**Pipelines:**
Sequential, Parallel, Gate

# Feature (new): Pipe expressions

This feature/+refactoring suggests adding data manipulation methods directly to `Pipe` objects, instead of writing separate classes for each type of processing step.

In the following example we showcase how this approach might play out in practice.
```python
# Current way of planning multiple pipe operations
batch = ...
tokenizer = ...
plan = Sequential(
    FilterKeys(),
    TokenizerPipe(tokenizer, key='text'),
    AddPrefix(),
    DropKeys(keys=['k1', 'k2', 'k3'])
)
plan(batch)

# Polars inspired way of planning
batch = ...
tokenizer = ...
plan = (
    Sequential(batch)
    .filter()
    .tokenize(tokenizer, key='text')
    .add_prefix()
    .drop(keys = ['k1', 'k2', 'k3'])
)
plan()
```

Adding another `dispatch` to the `__call__` method of `Pipe` for Polars `DataFrame` and `LazyFrame` types is also an option that may be helpful in further investigation.

# Feature (new): Query plan
The query plan allows us to visualize and optimize pipelines. This allows for debugging of steps before initiating computation. Furthermore, it allows us to optimize the plan similar to how package-managers are doing.

## Simple Ascii representation (hierarchical or diagram)

```python
plan.describe_plan()
```

## Graphiz visualization

```python
plan1.show_graph(optimized=False)
```

## Optimize plan

Here the optimization plan examines the steps and finds that it is silly to add prefixes for all entries
in the batch, if we are to filter it anyway later in the plan.

```python
tokenizer = ...
batch = ...
plan = (
    Sequential(batch)
    .tokenize(tokenizer, key='text')
    #.add_prefix() !<< this step is moved one step down
    #.filter()
    .filter()
    .add_prefix()
    .drop(keys = ['k1', 'k2', 'k3'])
)

q.lazy().describe_optimized_plan()
```

## Feature (new): Bridging between HuggingFace and LangChain 
TODO

<!-- 
```python
tokenizer = ...

# Pipeline planning
query_plan = (
    Sequential(
        TokenizerPipe(
            tokenizer,
            key="text",
            field="document",
            return_offsets_mapping=True,
            add_special_tokens=False,
            update=True,
        ),
        input_filter=HasPrefix("document"),
    ),
    Sequential(
        TokenizerPipe(
            tokenizer,
            key="text",
            field="title",
            return_offsets_mapping=True,
            add_special_tokens=False,
            update=True,
        ),
        input_filter=HasPrefix("title"),
    ),
    update=True,
)#.lazy() # admitting .lazy() will result in direct computation

# TODO: is partial(query_plan) the equivalent in this case?

# Execution plan
plan = Parallel(query_plan)
# > Sequential(query_plan)
``` -->
